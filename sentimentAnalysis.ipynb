{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1s-DApCJqYqwQDLpu7ZZBRVweeiu397mn",
      "authorship_tag": "ABX9TyOzYyH1tuU2kQ8VNgn8qD8H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhavan1501/sentimentAnalysis/blob/main/sentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Header Files**"
      ],
      "metadata": {
        "id": "cpsVfdgMboCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "# from numpy import array\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0zdn9KLatNS0",
        "outputId": "f058f0ad-922f-45af-8f36-ce203f567216"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reading the data**"
      ],
      "metadata": {
        "id": "UQbrvDN2cKi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"/content/Restaurant_Reviews 1.tsv\",delimiter=\"\\t\",quoting=3)\n",
        "# data=pd.read_csv(\"/content/twitter_training.csv\")\n",
        "# data = pd.read_csv(\"/content/twitter_training.csv\", encoding='latin-1') # or 'ISO-8859-1', or 'cp1252'\n",
        "# print(data.head())\n",
        "# print(data.shape)\n",
        "# print(data.info())\n",
        "# print(data)\n",
        "# limitizer=WordNetLemmatizer()\n",
        "porterStem=PorterStemmer()\n",
        "stopWords=set(stopwords.words(\"english\"))\n",
        "# print (stopWords)"
      ],
      "metadata": {
        "id": "HHLjyvp0v6qC"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pre-Processing the data**\n"
      ],
      "metadata": {
        "id": "tUNm-e5-cbO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def processingData (text):\n",
        "  text=text.lower()\n",
        "  text=re.sub(r\"[^a-zA-Z]\",\" \",text)\n",
        "  tokens=word_tokenize(text)\n",
        "  # tokens=text.spilt()\n",
        "  tokens=[i for i in tokens if i not in stopWords]\n",
        "  tokens=[porterStem.stem(i) for i in tokens]\n",
        "\n",
        "  return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "CNKMlHF70eG1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transforming the data into Matrix**\n"
      ],
      "metadata": {
        "id": "bIimHycncl8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in data:\n",
        "#   processedData=array(processingData(i))\n",
        "# processedData=[processingData(i) for i in data]\n",
        "\n",
        "processedData=[]\n",
        "for i in range(0,1000):\n",
        "  processedData.append(processingData(data['Review'][i]))\n",
        "\n",
        "vector=CountVectorizer()\n",
        "x=vector.fit_transform(processedData).toarray()\n",
        "\n",
        "# print (processedData)\n",
        "# print(\"filtered words:\",vector.get_feature_names_out())\n",
        "# print(\"matrix\",x.toarray())\n",
        "# print (x)"
      ],
      "metadata": {
        "id": "SUEfMQAJ02HM",
        "collapsed": true
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **storing output values in y**\n"
      ],
      "metadata": {
        "id": "S9tc3_k53QW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y=data.iloc[:,-1].values\n",
        "# print(y.shape)"
      ],
      "metadata": {
        "id": "YMvsF9Bt0D_6"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **using train test split function**"
      ],
      "metadata": {
        "id": "92kAlc20Fvz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# temp=train_test_split(x,y,test_size=0.2,random_state=25)\n",
        "# print(temp)\n",
        "xTrain,xTest,yTrain,yTest=train_test_split(x,y,test_size=0.2,random_state=25)"
      ],
      "metadata": {
        "id": "W6zp4zsZFug_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def algo_model(algo):\n",
        "  algo.fit(xTrain,yTrain)\n",
        "  yPredict=algo.predict(xTest)\n",
        "  accuracy=accuracy_score(yTest,yPredict)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "D89yuzrr1YPt"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Applying MultinomialNB Alogirithm**\n"
      ],
      "metadata": {
        "id": "dEPZErGQw6N2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"accuracy:\",algo_model(MultinomialNB()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f_3OntYcGn-",
        "outputId": "3dedefa0-3a2d-4178-d6f3-b6fecf041501"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trying other Alogorithms to get better accuracy**"
      ],
      "metadata": {
        "id": "tzk6uT0UyIRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LavnYYBDyGc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "algorithms = {\n",
        "    \"LinearSVC\": LinearSVC(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Bernoulli Naive Bayes\": BernoulliNB(),\n",
        "    \"XGBoost\": XGBClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"GaussianNB\": GaussianNB(),\n",
        "    }"
      ],
      "metadata": {
        "id": "oUG_5oV420ki"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(algorithms.items())\n",
        "for name, algo in algorithms.items():\n",
        "    # accuracy = algo_model(algo)\n",
        "    print(f\"{name} Accuracy: {algo_model(algo)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMbv2h7E35JH",
        "outputId": "358b45e8-b489-4fed-e714-2d651c20c75a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC Accuracy: 0.76\n",
            "Random Forest Accuracy: 0.765\n",
            "Logistic Regression Accuracy: 0.75\n",
            "Bernoulli Naive Bayes Accuracy: 0.74\n",
            "XGBoost Accuracy: 0.735\n",
            "Decision Tree Accuracy: 0.725\n",
            "GaussianNB Accuracy: 0.665\n"
          ]
        }
      ]
    }
  ]
}